{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                      Imagen Médica              -                Máster Visión Artificial (2023-2024)\n",
    "\n",
    "**María Cornejo Antonaya (m.cornejo.2023@alumnos.urjc.es)**\n",
    "\n",
    "**Nuria Miralles Gavara (n.miralles.2023@alumnos.urjc.es)**\n",
    "\n",
    "**Juan Montes Cano (juan.montes@urjc.es)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1: Filtrado y Mejora de Imagen Médica\n",
    "### Introducción\n",
    "El objetivo de la práctica es evaluar tres algoritmos de filtrado de imagen médica junto a la creación de un mapa paramétrico de estas imágenes. El resultado esperado es la mejora de la calidad de la imagen original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filtrado anisotrópico por difusión\n",
    "El filtrado anisotrópico por difusión es una técnica que persigue eliminar el ruido presente en las zonas homogéneas de una imagen, preservando simultáneamente los contornos que delimitan las distintas regiones. Este filtro logra una notable mejora en la calidad de la imagen al eliminar las imperfecciones sin comprometer la nitidez de la misma.\n",
    "En este ejercicio se configura el filtro anisotrópico para las imágenes *T1.png* y *T2.png*. Con el objetivo de obtener resultados más evidentes, se ha añadido ruido Rayleigh a las imágenes proporcionadas. Este tipo de ruido es característico del procesamiento de imágenes de resonancia magnética (MRI).\n",
    "La función de densidad de probabilidad de este ruido se define como:\n",
    "\n",
    "\n",
    "$$ P(q) = q \\sigma^2 e^{-\\frac{1}{2} \\frac{q^2}{\\sigma^2}} $$\n",
    "\n",
    "La implementación del filtro anisotrópico se ha llevado a cabo mediante el algoritmo de Perona-Malik para suavizar imágenes. El código correspondiente se encuentra disponible en el siguiente repositorio: \n",
    "[Repositorio](https://github.com/krishanuskr/ImageRestoration/blob/master/imagerestoration.py)\n",
    "\n",
    "Este filtro está definido por un conjunto de parámetros cuya configuración permite realizar un filtrado específico para cada imagen. A continuación, se procede a analizar cada uno de estos parámetros para su posterior ajuste óptimo para ambas imágenes.\n",
    "Parámetros:\n",
    "- *\"img\"*: Imagen de entrada que se desea filtrar.\n",
    "- *\"K\"*: Coeficiente de conductancia utilizado para medir la sensibilidad del algoritmo frente a los detalles de la imagen en ambas funciones, exponencial y Cauchy.\n",
    "- *\"LAMBDA\"*: Parámetro cuyo valor máximo es 0.25 asegurando que la imagen resultante sea estable. Su valor cuantifica la difusión aplicada en cada iteración del algoritmo. Por lo tanto, a medida que aumenta su valor, la imagen será más suave.\n",
    "- *\"gfunction\"*: Selección de la función de difusión a utilizar. Se distingue entre la función exponencial *(‘Exponential’)* y la función de Cauchy *(‘Cauchy’)*. La primera opción tiene mayor sensibilidad a cambios bruscos en la intensidad de la imagen proporcionando un mayor suavizado en áreas homogéneas. Por otro lado, la segunda opción es menos sensible en comparación con la función exponencial y favorece las regiones grandes sobre las pequeñas.\n",
    "- *\"“nIterations\"*:  Número de interacciones. A medida que se aumenta su valor, la imagen de salida se suaviza más, ya que los efectos de la difusión se propagan sobre un mayor número de iteraciones. \n",
    "\n",
    "##### Evaluación del parámetro *K*\n",
    "\n",
    "<img src=\"extremos_K.png\" alt=\"image\" width=\"700\"/>\n",
    "\n",
    "#### Evaluación del parámetro *$\\lambda$*\n",
    "<img src=\"extremos_lambda.png\" alt=\"image\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## APARTADO P1 Y P2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def PeronaMalik_Smoother(image,K,LAMBDA,gfunction,nIterations,convert_to_grayscale=True): \n",
    "    # REFERENCE: https://github.com/krishanuskr/ImageRestoration/blob/master/imagerestoration.py\n",
    "    \n",
    "    print ('Image shape: ', image.shape)\n",
    "\n",
    "    \n",
    "    if convert_to_grayscale == True:\n",
    "        try:\n",
    "            image = np.mean(image,axis=2)\n",
    "        except IndexError:\n",
    "            raise IndexError('Image is already 2D, so is assumed grayscale already. Check shape.')\n",
    "\n",
    "    \n",
    "    #Get number of color channels of image [1 for grayscale, 3 for RGB, 4 for RGBA]\n",
    "    #Assuming the image shape is heigth x width for grayscale,\n",
    "    #or heigth x width x Nchannels for color. But not expecting more than 3 dimensions.\n",
    "    nChannels = 1 if image.ndim == 2 else image.shape[2]\n",
    "    print ('nChannels',nChannels)\n",
    "    \n",
    "    #In the case of a grayscale image, to make things easier later, just make the grayscale image have a 3rd axis of length 1\n",
    "    if nChannels == 1:\n",
    "        image = np.expand_dims(image,axis=2)\n",
    "    \n",
    "    #4D Container array of all iterations of image diffusion\n",
    "    image_stack = np.expand_dims(image,axis=0)\n",
    "    \n",
    "    #Do nIterations of diffusion:\n",
    "    for i in range(nIterations):\n",
    "        if i % 10 == 0:\n",
    "            print ('Starting iteration {0} of {1}'.format(i,nIterations))\n",
    "        image_t = np.zeros(image.shape)\n",
    "        for channel in range(nChannels):\n",
    "            temp = image_stack[-1][:,:,channel]\n",
    "            \n",
    "            #Following equation 8 in paper: calculate nearest neighbor differences to approximate gradient of image intensity\n",
    "            vert_diff = np.diff(temp,axis=0)\n",
    "            horiz_diff = np.diff(temp,axis=1)\n",
    "            nanrow = np.expand_dims(np.nan*np.ones(vert_diff.shape[1]),axis=0)\n",
    "            nancol = np.expand_dims(np.nan*np.ones(horiz_diff.shape[0]),axis=0).T\n",
    "            grad_S = np.vstack((vert_diff,nanrow)) #NaN on bottom row\n",
    "            grad_N = np.vstack((nanrow,-vert_diff)) #NaN on top row, and negated diffs since going opposite direction from np.diff() default\n",
    "            grad_E = np.hstack((horiz_diff,nancol)) #NaN on right column\n",
    "            grad_W = np.hstack((nancol,-horiz_diff)) #NaN on left column, and negated diffs since going opposite direction from np.diff() default\n",
    "            \n",
    "            #Following equation 10 in paper: calculate conduction coefficients\n",
    "            #Technically, the coefficients should be more appropriately be evaluated at the halfway point between pixels, not at the pixels themselves.\n",
    "            #But this is more complicated for approximately same results (according to authors). So use the same values for gradients as above.\n",
    "            if gfunction == 'Exponential':\n",
    "                c_S = np.exp(-(grad_S/K)**2)\n",
    "                c_N = np.exp(-(grad_N/K)**2)\n",
    "                c_E = np.exp(-(grad_E/K)**2)\n",
    "                c_W = np.exp(-(grad_W/K)**2)\n",
    "                \n",
    "            if gfunction == 'Cauchy':\n",
    "                c_S = 1./(1.+(grad_S/K)**2)\n",
    "                c_N = 1./(1.+(grad_N/K)**2)\n",
    "                c_E = 1./(1.+(grad_E/K)**2)\n",
    "                c_W = 1./(1.+(grad_W/K)**2)\n",
    "            \n",
    "            #Examine the conduction coefficients:\n",
    "            \n",
    "            #Following equation 7 in paper: Update the image using the diffusion equation:\n",
    "            temp2 = temp + LAMBDA*(c_S*grad_S + c_N*grad_N + c_E*grad_E + c_W*grad_W)\n",
    "            \n",
    "            #Reset boundaries since the paper uses adiabatic boundary conditions and above steps intentionally set boudnaries to NaNs\n",
    "            temp2[:,0] = temp[:,0] #Left edge\n",
    "            temp2[:,-1] = temp[:,-1] #Right edge\n",
    "            temp2[-1,:] = temp[-1,:] #Bottom edge\n",
    "            temp2[0,:] = temp[0,:] #Top edge\n",
    "            \n",
    "            #Update this channel of the image at this time step\n",
    "            image_t[:,:,channel] = temp2\n",
    "\n",
    "        image_t = np.expand_dims(image_t,axis=0)\n",
    "        image_stack = np.append(image_stack,image_t,axis=0)    \n",
    "    \n",
    "    \n",
    "    #image_stack is stack of all iterations.\n",
    "    #iteration 0 is original image, iteration -1 is final image.\n",
    "    #Intermediate images are also returned for visualization and diagnostics\n",
    "    return image_stack\n",
    "\n",
    "\n",
    "### MULTICHANNEL\n",
    "\n",
    "\n",
    "def PeronaMalik_Smoother_MC(image,image1,K,LAMBDA,gfunction,nIterations,convert_to_grayscale=True): # REFERENCE: https://github.com/krishanuskr/ImageRestoration/blob/master/imagerestoration.py\n",
    "    \n",
    "    print ('Image shape: ', image.shape)\n",
    "\n",
    "    \n",
    "    if convert_to_grayscale == True:\n",
    "        try:\n",
    "            image = np.mean(image,axis=2)\n",
    "        except IndexError:\n",
    "            raise IndexError('Image is already 2D, so is assumed grayscale already. Check shape.')\n",
    "\n",
    "    \n",
    "    #Get number of color channels of image [1 for grayscale, 3 for RGB, 4 for RGBA]\n",
    "    #Assuming the image shape is heigth x width for grayscale,\n",
    "    #or heigth x width x Nchannels for color. But not expecting more than 3 dimensions.\n",
    "    nChannels = 1 if image.ndim == 2 else image.shape[2]\n",
    "    print ('nChannels',nChannels)\n",
    "    \n",
    "    #In the case of a grayscale image, to make things easier later, just make the grayscale image have a 3rd axis of length 1\n",
    "    if nChannels == 1:\n",
    "        image = np.expand_dims(image,axis=2)\n",
    "        image1 = np.expand_dims(image1,axis=2)\n",
    "    \n",
    "    #4D Container array of all iterations of image diffusion\n",
    "    image_stack = np.expand_dims(image,axis=0)\n",
    "    image_stack_1 = np.expand_dims(image1,axis=0)\n",
    "\n",
    "    #Do nIterations of diffusion:\n",
    "    for i in range(nIterations):\n",
    "        if i % 10 == 0:\n",
    "            print ('Starting iteration {0} of {1}'.format(i,nIterations))\n",
    "        image_t = np.zeros(image.shape)\n",
    "        image_t1 = np.zeros(image1.shape)\n",
    "        for channel in range(nChannels):\n",
    "            temp = image_stack[-1][:,:,channel]\n",
    "            temp1 = image_stack_1[-1][:,:,channel]\n",
    "\n",
    "            #Following equation 8 in paper: calculate nearest neighbor differences to approximate gradient of image intensity\n",
    "            vert_diff = np.diff(temp,axis=0)\n",
    "            vert_diff1 = np.diff(temp1,axis=0)\n",
    "            horiz_diff = np.diff(temp,axis=1)\n",
    "            horiz_diff1 = np.diff(temp1,axis=1)\n",
    "            nanrow = np.expand_dims(np.nan*np.ones(vert_diff.shape[1]),axis=0)\n",
    "            nanrow1 = np.expand_dims(np.nan*np.ones(vert_diff1.shape[1]),axis=0)\n",
    "            nancol = np.expand_dims(np.nan*np.ones(horiz_diff.shape[0]),axis=0).T\n",
    "            nancol1 = np.expand_dims(np.nan*np.ones(horiz_diff1.shape[0]),axis=0).T\n",
    "            grad_S = np.vstack((vert_diff,nanrow)) #NaN on bottom row\n",
    "            grad_S1 = np.vstack((vert_diff1,nanrow1)) #NaN on bottom row\n",
    "            grad_N = np.vstack((nanrow,-vert_diff)) #NaN on top row, and negated diffs since going opposite direction from np.diff() default\n",
    "            grad_N1 = np.vstack((nanrow1,-vert_diff1)) #NaN on top row, and negated diffs since going opposite direction from np.diff() default\n",
    "            grad_E = np.hstack((horiz_diff,nancol)) #NaN on right column\n",
    "            grad_E1 = np.hstack((horiz_diff1,nancol1)) #NaN on right column\n",
    "            grad_W = np.hstack((nancol,-horiz_diff)) #NaN on left column, and negated diffs since going opposite direction from np.diff() default\n",
    "            grad_W1 = np.hstack((nancol1,-horiz_diff1)) #NaN on left column, and negated diffs since going opposite direction from np.diff() default\n",
    "\n",
    "            #Recalculamos los gradientes como dice en el pdf\n",
    "            grad_W = (grad_W**2 + grad_W1**2)**0.5\n",
    "            grad_E = (grad_E**2 + grad_E1**2)**0.5\n",
    "            grad_N = (grad_N**2 + grad_N1**2)**0.5\n",
    "            grad_S = (grad_S**2 + grad_S1**2)**0.5\n",
    "            \n",
    "            #Following equation 10 in paper: calculate conduction coefficients\n",
    "            #Technically, the coefficients should be more appropriately be evaluated at the halfway point between pixels, not at the pixels themselves.\n",
    "            #But this is more complicated for approximately same results (according to authors). So use the same values for gradients as above.\n",
    "            if gfunction == 'Exponential':\n",
    "                c_S = np.exp(-(grad_S/K)**2)\n",
    "                c_N = np.exp(-(grad_N/K)**2)\n",
    "                c_E = np.exp(-(grad_E/K)**2)\n",
    "                c_W = np.exp(-(grad_W/K)**2)\n",
    "                \n",
    "            if gfunction == 'Cauchy':\n",
    "                c_S = 1./(1.+(grad_S/K)**2)\n",
    "                c_N = 1./(1.+(grad_N/K)**2)\n",
    "                c_E = 1./(1.+(grad_E/K)**2)\n",
    "                c_W = 1./(1.+(grad_W/K)**2)\n",
    "            \n",
    "            #Examine the conduction coefficients:\n",
    "            \n",
    "            #Following equation 7 in paper: Update the image using the diffusion equation:\n",
    "            temp2 = temp + LAMBDA*(c_S*grad_S + c_N*grad_N + c_E*grad_E + c_W*grad_W)\n",
    "            temp3 = temp1 + LAMBDA*(c_S*grad_S + c_N*grad_N + c_E*grad_E + c_W*grad_W)\n",
    "            #Reset boundaries since the paper uses adiabatic boundary conditions and above steps intentionally set boudnaries to NaNs\n",
    "            temp2[:,0] = temp[:,0] #Left edge\n",
    "            temp3[:,0] = temp1[:,0] #Left edge\n",
    "            temp2[:,-1] = temp[:,-1] #Right edge\n",
    "            temp3[:,-1] = temp1[:,-1] #Right edge\n",
    "            temp2[-1,:] = temp[-1,:] #Bottom edge\n",
    "            temp3[-1,:] = temp1[-1,:] #Bottom edge\n",
    "            temp2[0,:] = temp[0,:] #Top edge\n",
    "            temp3[0,:] = temp1[0,:] #Top edge\n",
    "            \n",
    "            #Update this channel of the image at this time step\n",
    "            image_t[:,:,channel] = temp2\n",
    "            image_t1[:,:,channel] = temp3\n",
    "\n",
    "        image_t = np.expand_dims(image_t,axis=0)\n",
    "        image_t1 = np.expand_dims(image_t1,axis=0)\n",
    "        image_stack = np.append(image_stack,image_t,axis=0)\n",
    "        image_stack_1 = np.append(image_stack_1,image_t1,axis=0)\n",
    "    \n",
    "    \n",
    "    #image_stack is stack of all iterations.\n",
    "    #iteration 0 is original image, iteration -1 is final image.\n",
    "    #Intermediate images are also returned for visualization and diagnostics\n",
    "    return image_stack,image_stack_1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Load test image\n",
    "    image = mpimg.imread('Material_P1/T2.png')\n",
    "    image1 = mpimg.imread('Material_P1/T1.png')\n",
    "    #Set algorithm parameters\n",
    "    K = [1]\n",
    "    LAMBDA = [0.15]\n",
    "    nIterations = [5] #100\n",
    "    gfunction = 'Exponential' #'Cauchy'\n",
    "\n",
    "\n",
    "    \n",
    "    #plt.title('Original',fontsize=30)\n",
    "    #plt.imshow(image,interpolation='None',cmap='gray')\n",
    "    #plt.show()\n",
    "\n",
    "    #Add noise to image\n",
    "    noise = np.random.normal(0,.01,image.shape)\n",
    "    #image = image + noise\n",
    "\n",
    "    #Add rician noise to the image\n",
    "    noise = np.random.rayleigh(0.05,image.shape)\n",
    "    image = image + noise\n",
    "    \n",
    "    plt.title('Original + Noise',fontsize=30)\n",
    "    plt.imshow(image,interpolation='None',cmap='gray')\n",
    "    #plt.show()\n",
    "    #Plot grayscale example\n",
    "    #Plot in the same plot the images with different K and Lambda values\n",
    "\n",
    "    # Create a 6x4 grid plot\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 18))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].imshow(image,interpolation='None',cmap='gray')\n",
    "    axes[0].set_title('Original + Noise',fontsize=10)\n",
    "    axes[1].imshow(image1,interpolation='None',cmap='gray')\n",
    "    axes[1].set_title('Original + Noise',fontsize=10)\n",
    "\n",
    "    cont = 2\n",
    "    for  k in K:\n",
    "        for l in LAMBDA:\n",
    "            for iteraciones in nIterations:\n",
    "                for funcion in ['Cauchy']:\n",
    "                    \n",
    "                    PMimage_stack1,PMimage_stack2 = PeronaMalik_Smoother_MC(image,image1,k,l,funcion,iteraciones,convert_to_grayscale=False)\n",
    "                    #I want to save each image into the axes array\n",
    "                    axes[cont].set_title('K={0}, Lambda={1}, Iter={2}, Func={3}'.format(k,l,iteraciones,funcion),fontsize=10)\n",
    "                    #I want it to be grayscale\n",
    "                    axes[cont].imshow(np.squeeze(PMimage_stack1[-1]),interpolation='None',cmap='gray')\n",
    "                    cont+=1\n",
    "                    axes[cont].set_title('K={0}, Lambda={1}, Iter={2}, Func={3}'.format(k,l,iteraciones,funcion),fontsize=10)\n",
    "                    axes[cont].imshow(np.squeeze(PMimage_stack2[-1]),interpolation='None',cmap='gray')\n",
    "                    axes[cont].axis('off')\n",
    "                    cont+=1\n",
    "                    print(\"ITERATION: \",cont)\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Partiendo del objetivo de aplicar el filtro anisotrópico para eliminar el ruido y preservar los bordes principales, dado que no se busca resaltar el detalle sino la forma, podemos concluir que se ha logrado alcanzar el objetivo principal. El filtro difumina las texturas homogéneas, eliminando el ruido Rayleigh, al mismo tiempo que conserva los bordes de las imágenes de entrada.\n",
    "\n",
    "Parámetros del filtro anisotrópico para la imagen *“T1.png”* + Rayleigh (0.1):\n",
    "*K = 15; LAMBDA = 0.03; gfunction = Exponencial; nIterations = 20.*\n",
    "\n",
    "##### Resultado T1\n",
    "<img src=\"resultado_T1.png\" alt=\"image\" width=\"700\"/>\n",
    "\n",
    "Parámetros del filtro anisotrópico para la imagen *“T2.png”* + Rayleigh (0.05):\n",
    "*K = 1; LAMBDA = 0.15; gfunction = Exponencial; nIterations = 5.*\n",
    "\n",
    "##### Resultado T2\n",
    "<img src=\"resultado_T2.png\" alt=\"image\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P2. Filtrado al caso de dos imágenes\n",
    "\n",
    "Cuanto más iteraciones más grado de importancia tiene el gradiente calculado \n",
    "los bordes se mantienen\n",
    "Comparando ambos resultados podemos concluir \n",
    "se centra más en la estructura de la imagen es decir respetar los bordes y las regiones lo pilla mejor pero en cambio ensucia más imagen  perdiendo los detalles\n",
    "Comparando el resultado con el monocanal este respeta pero los bordes haciéndolos más nítidos mientras que en la segunda respeta mejor los bordes pero pierde la homogeneidad ganada en el caso de monocanal\n",
    "\n",
    "##### Evaluación multicanal\n",
    "<img src=\"multicanal.png\" alt=\"image\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filtrado no local de media (Non-local means)\n",
    "En este ejercicio, se procede a implementar un algoritmo de filtrado no local de media (Non-local means) para imágenes médicas. \n",
    "[Repositorio de referencia](https://github.com/praveenVnktsh/Non-Local-Means/blob/main/main.py)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import dif_aniso as da\n",
    "\n",
    "def nonLocalMeans(noisy, params = tuple(), verbose = True): ### REFERENCIA : https://github.com/praveenVnktsh/Non-Local-Means/blob/main/main.py\n",
    "  '''\n",
    "  Performs the non-local-means algorithm given a noisy image.\n",
    "  params is a tuple with:\n",
    "  params = (bigWindowSize, smallWindowSize, h)\n",
    "  Please keep bigWindowSize and smallWindowSize as even numbers\n",
    "  '''\n",
    "\n",
    "  bigWindowSize, smallWindowSize, h  = params\n",
    "  padwidth = bigWindowSize//2\n",
    "  image = noisy.copy()\n",
    "\n",
    "  # The next few lines creates a padded image that reflects the border so that the big window can be accomodated through the loop\n",
    "  paddedImage = np.zeros((image.shape[0] + bigWindowSize,image.shape[1] + bigWindowSize))\n",
    "  paddedImage = paddedImage.astype(np.uint8)\n",
    "  paddedImage[padwidth:padwidth+image.shape[0], padwidth:padwidth+image.shape[1]] = image\n",
    "  paddedImage[padwidth:padwidth+image.shape[0], 0:padwidth] = np.fliplr(image[:,0:padwidth])\n",
    "  paddedImage[padwidth:padwidth+image.shape[0], image.shape[1]+padwidth:image.shape[1]+2*padwidth] = np.fliplr(image[:,image.shape[1]-padwidth:image.shape[1]])\n",
    "  paddedImage[0:padwidth,:] = np.flipud(paddedImage[padwidth:2*padwidth,:])\n",
    "  paddedImage[padwidth+image.shape[0]:2*padwidth+image.shape[0], :] =np.flipud(paddedImage[paddedImage.shape[0] - 2*padwidth:paddedImage.shape[0] - padwidth,:])\n",
    "  \n",
    "\n",
    "\n",
    "  iterator = 0\n",
    "  totalIterations = image.shape[1]*image.shape[0]*(bigWindowSize - smallWindowSize)**2\n",
    "\n",
    "  if verbose:\n",
    "    print(\"TOTAL ITERATIONS = \", totalIterations)\n",
    "\n",
    "  outputImage = paddedImage.copy()\n",
    "\n",
    "  smallhalfwidth = smallWindowSize//2\n",
    "\n",
    "\n",
    "  # For each pixel in the actual image, find a area around the pixel that needs to be compared\n",
    "  for imageX in range(padwidth, padwidth + image.shape[1]):\n",
    "    for imageY in range(padwidth, padwidth + image.shape[0]):\n",
    "      \n",
    "      bWinX = imageX - padwidth\n",
    "      bWinY = imageY - padwidth\n",
    "\n",
    "      #comparison neighbourhood\n",
    "      compNbhd = paddedImage[imageY - smallhalfwidth:imageY + smallhalfwidth + 1,imageX-smallhalfwidth:imageX+smallhalfwidth + 1]\n",
    "      \n",
    "      \n",
    "      pixelColor = 0\n",
    "      totalWeight = 0\n",
    "\n",
    "      # For each comparison neighbourhood, search for all small windows within a large box, and compute their weights\n",
    "      for sWinX in range(bWinX, bWinX + bigWindowSize - smallWindowSize, 1):\n",
    "        for sWinY in range(bWinY, bWinY + bigWindowSize - smallWindowSize, 1):   \n",
    "          #find the small box       \n",
    "          smallNbhd = paddedImage[sWinY:sWinY+smallWindowSize + 1,sWinX:sWinX+smallWindowSize + 1]\n",
    "          euclideanDistance = np.sqrt(np.sum(np.square(smallNbhd - compNbhd)))\n",
    "          #weight is computed as a weighted softmax over the euclidean distances\n",
    "          weight = np.exp(-euclideanDistance/h)\n",
    "          totalWeight += weight\n",
    "          pixelColor += weight*paddedImage[sWinY + smallhalfwidth, sWinX + smallhalfwidth]\n",
    "          iterator += 1\n",
    "\n",
    "          if verbose:\n",
    "            percentComplete = iterator*100/totalIterations\n",
    "            if percentComplete % 5 == 0:\n",
    "              print('% COMPLETE = ', percentComplete)\n",
    "\n",
    "      pixelColor /= totalWeight\n",
    "      outputImage[imageY, imageX] = pixelColor\n",
    "\n",
    "  return outputImage[padwidth:padwidth+image.shape[0],padwidth:padwidth+image.shape[1]]\n",
    "\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, std_dev=0.5):\n",
    "    \"\"\"\n",
    "    Agrega ruido gaussiano a una imagen.\n",
    "    \"\"\"\n",
    "    noisy_image = np.copy(image)\n",
    "    h, w = image.shape\n",
    "    noise = np.random.normal(mean, std_dev, (h, w)).astype(np.uint8)\n",
    "    noisy_image = cv2.add(image, noise)\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "# Cargar la imagen\n",
    "original_image = cv2.imread('Material_P1/T1.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Agregar ruido gaussiano a la imagen\n",
    "noisy_image = add_gaussian_noise(original_image)\n",
    "\n",
    "gParams = {\n",
    "    'bigWindow' : 20,\n",
    "    'smallWindow':6,\n",
    "    'h':14,\n",
    "}\n",
    "imagen_filtrogaussiano = cv2.GaussianBlur(noisy_image, (5, 5), 0)\n",
    "anisotropic_img = da.PeronaMalik_Smoother(noisy_image, 5, 0.01, \"Exponential\",15,False)[-1]\n",
    "#perform NLM filtering\n",
    "filtered_image = nonLocalMeans(noisy_image, params = (gParams['bigWindow'], gParams['smallWindow'],gParams['h']))\n",
    "\n",
    "# Mostrar las imágenes\n",
    "plt.figure(figsize=(15, 5))\n",
    "#\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title('Imagen Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(noisy_image, cmap='gray')\n",
    "plt.title('Imagen con Ruido Gaussiano')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.imshow(filtered_image, cmap='gray')\n",
    "plt.title('Imagen Filtrada con NLM')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(anisotropic_img, cmap='gray')\n",
    "plt.title('Imagen Filtrada con Perona Malik')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(imagen_filtrogaussiano, cmap='gray')\n",
    "plt.title('Imagen Filtrada con Filtro Gaussiano')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3. Filtrado non-local means\n",
    "El filtrado Non-local means es una técnica utilizada en el procesamiento de imágenes para reducir el ruido y mejorar la calidad de las imágenes. A diferencia de otros métodos de filtrado que se centran en la información local de cada píxel, el filtrado non-local means busca similitudes en regiones más amplias de la imagen.\n",
    "\n",
    "Este se basa en sustituir el valor de cada píxel por la media ponderada de los píxeles en la imagen que son considerados similares a través de una métrica de similitud. Esta similitud se mide comparando vecindarios de píxeles alrededor de cada píxel. Cuanto más similar sea el vecindario de un píxel con otros vecindarios en la imagen, mayor será su contribución en el cálculo de la media para ese píxel.\n",
    "\n",
    "Para probar el algoritmo se ha usado la imagen T1 y, al igual que en el ejercicio anterior, la imagen de entrada se ha preprocesado para añadirle un ruido de tipo gaussiano con media 0 y desviación estándar 0.5.\n",
    "\n",
    "<img src=\"noised_T1.png\" alt=\"image\" width=\"300\"/>\n",
    "<img src=\"noised_T2.png\" alt=\"image\" width=\"300\"/>\n",
    "\n",
    "### P4. Comparar resultado de filtrar imagen T1 con este filtro, filtrado gaussiano y filtro de Perona-Malik\n",
    "\n",
    "El filtrado gaussiano, implementado mediante la función cv2.GaussianBlur de OpenCV, se utiliza para desenfocar la imagen. Cuanto mayor sea la desviación estándar especificada en el núcleo gaussiano, mayor será el desenfoque aplicado a la imagen. Esto resulta en una imagen más suavizada, donde los bordes se vuelven menos definidos y la imagen se vuelve más homogénea. A medida que aumenta la desviación estándar, se pierden los detalles finos y la imagen se vuelve más borrosa.\n",
    "\n",
    "El filtrado anisotrópico o de Perona-Malik, implementado en el ejercicio P1, es una función dependiente del gradiente que se utiliza para realizar los bordes. En este caso, no da buenos resultados apenas consigue disminuir el ruido gaussiano de la imagen.\n",
    "\n",
    "Por otro lado, el filtrado Non-local means, o medios no locales, a diferencia del filtrado gaussiano que se centra en la información local de cada píxel, busca similitudes en regiones más amplias de la imagen. Esto permite una reducción del ruido más efectiva mientras se conservan los detalles finos y los bordes. En comparación con el filtrado gaussiano, el filtrado non-local means produce una imagen más nítida y con bordes mejor definidos, lo que da a la imagen una apariencia mejorada y más \"reluciente\"\n",
    "\n",
    "<img src=\"P4.png\" alt=\"image\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cálculo de mapas de hierro del cerebro\n",
    "\n",
    "En este apartado se persigue medición no invasiva de la presencia de hierro\n",
    "mediante resonancia magnética. \n",
    "La imagen de resonancia magnética utiliza pulsos de radiofrecuencia\n",
    "ajustados al giro de los protones, dando lugar a una velocidad de precisión\n",
    "que define el movimiento circular rotatorio de los núcleos atómicos al someterse\n",
    "a un campo magnético.\n",
    "\n",
    "Esta velocidad es distinta para el hierro, ya que su entorno químico\n",
    "experimenta un campo magnético diferente que provoca un desfase representado\n",
    "por la constante *T2*. Este concepto resulta fundamental para la detección efectiva de \n",
    "la presencia de hierro.\n",
    "\n",
    "Partiendo de los datos de intensidad y el tiempo de eco de cada una de las imágenes facilitadas (“Hierro_TE*tif”), se procede al cálculo de los mapas de hierro del cerebro *(T2)*.\n",
    "A continuación, se muestra el análisis teórico empleado para la resolución del sistema de ecuaciones derivado de la definición de la señal en función del tiempo:\n",
    "\n",
    "<div id=\"eq1\">\n",
    "\n",
    "$$s(t) = s_0 \\cdot e^{-\\frac{t}{T_2}}; (1)$$ \n",
    "</div>\n",
    "\n",
    "Siendo:\n",
    "\n",
    "*“s”*: Intensidad de los píxeles de la adquisición\n",
    "\n",
    " *“t”*: Tiempo de eco\n",
    "\n",
    "*“$s_0$”*: Señal inicial \n",
    "\n",
    "*“$T_2$”*: Tiempo de relajación\n",
    "\n",
    "\n",
    "Para resolver este sistema, se ha empleado la técnica de optimización de mínimos cuadrados, expresada por la siguiente ecuación: \n",
    " \n",
    "<div id=\"eq2\">\n",
    "\n",
    "$$ y = A · e^{Bx}; (2) $$\n",
    "</div>\n",
    "\n",
    "Tomando el algoritmo, se obtiene:\n",
    "\n",
    "<div id=\"eq3\">\n",
    "\n",
    "$$ln(y) = ln(A) + Bx; (3)$$\n",
    "</div>\n",
    "\n",
    "Comparando con la ecuación ([1](#eq1)), se deduce:\n",
    "\n",
    "$$A = S_0;$$\n",
    "$$ B = R_2 = - 1/T_2; $$\n",
    "$$ x = TE; $$\n",
    "$$ y = S;$$\n",
    "\n",
    "<div id=\"eq4\">\n",
    "\n",
    "$$ln(S(t)) = ln(S_0) + R_2;  (4)$$  \n",
    "</div>\n",
    "\n",
    "Teniendo en cuenta estas equivalencias y las restricciones del algoritmo, se han excluido los valores de intensidad nulos para evitar la indeterminación del logaritmo de cero. \n",
    "Posteriormente, se muestra la implementación y solución de la ecuación ([4](#eq4))\n",
    "\n",
    "<img src=\"p5_imagenes.png\" alt=\"image\" width=\"300\"/>\n",
    "\n",
    "\n",
    "<img src=\"P5_heatmap.png\" alt=\"image\" width=\"300\"/>\n",
    "\n",
    "\n",
    "<img src=\"P5.png\" alt=\"image\" width=\"300\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
